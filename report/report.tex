\documentclass[a4paper, 9pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi)
\usepackage{fullpage} % changes the margin
\usepackage{enumitem}
\usepackage{hyperref}


\begin{document}
\noindent
\large\textbf{WIR project}

\section*{Sentiment analysis of Reddit posts and relative comments leading to conflicts}

\paragraph*{Introduction}
Our reference paper authors developed a model to classify posts containing cross-links (i.e. links that
point to another subreddit) according to the sentiment contained in their texts. Indeed they expect that 
overtly negative words reveal a conflict intent in the source posts. In this work we also consider the text
of comments assuming that if a post receives attention and the response of other members agrees in the 
intent of the post we have a further proof that a conflict may be generated by such activity.
The dataset we used as a ground truth was built with the help of Mechanical Turk crowd-workers that manually
annotated almost a thousand of cross-links telling whether the sentiment of the source post towards the
target post was negative or positive/neutral (they got an inter-rater agreement of 0.95). They then used
their model to expand the dataset. The model proposed by the authors is a Random Forest classifier with
forests of 400 trees and it achieves an accuracy of 0.8 on a 10-fold cross validation. In order to use this
model they employ a large set of features that by the way are not actually very well documented.
In this work we propose an improved model based on the bare content of the text in terms of words without 
any use of additional handcrafted features such as the ones employed in the original paper. This is a trend
in modern classification methods where the deep learning models substitute annoying feature engineering.
As we will see in the conclusions our model only knowing the text of posts and comments achieves an
improvement of 10 points over the results of the paper.

\paragraph{Getting the data}
We used the data made available by the original paper authors where each analysed post ID is given a label 
"burst" or "non-burst" corresponding to negative and positive/neutral sentiments. Given these post IDs we 
used the Pushshift API to the Reddit data in order to retrieve: the text of the title and of the body 
of the post, the top 8 comments (most relevant ones based on upvotes) taking from each a maximum of 512
characters so to avoid giving too much weight to the comments with respect to post text.
Here we encountered a first challenge due to the time constraints of the API access. Indeed retrieving the
data for a single post required a series of 3 sequential API calls and the delays given by the size of
the data in the Pushshift backend lead to a time for post download of almost 1 second. So we launched
parallel processes downloading and elaborating different batches of the original data that contained almost 
400000 posts. Unfortunately a large amount of API calls resulted in errors, but anyway we managed to get
enough data to obtain the aforementioned performances.
Indeed the data we got was composed predominantly (92-95\% as announced in the original paper) by items
labeled with positive sentiment. This was initially a problem in the classification task and we solved it 
with under-sampling techniques that revealed themselves to be really effective. 

\paragraph*{Deep learning model}
Our model is essentially based on a combination of a convolutional neural network followed by a LSTM 
recurrent network. At the basis of the model we use GloVe word embeddings that allow us to represent each
word in the text in a fixed-size, compact and dense vector of 300 floats also taking into account similarity
between words. The advantage of GloVe vectors over the simple one-hot representation of words is that these
vectors where trained using a neural network so that words that have a similar context are close in the
vector space, for example you expect that the distance in the vector space between the word-vectors for
"Paris" and "France" is very similar to the distance between "Rome" and "Italy". So an initial embedding
layer gives us the representation of each word before feeding it to the convolutional network.
Then as we just said the word embeddings are given in input to the convolutional layer. The purpose of this
layer is to capture more general patterns in the data helping the network to better generalize to new
examples without excessively specialising on the text of the training data. The result is given in input to
the bidirectional LSTM layer which is responsible of learning the patterns in the \emph{sequences} of words,
something that recurrent network were designed to do well. In particular the LSTM architecture allows to
"remember" longer sequences learning which parts of the sequences are more important. As for the
hyper-parameters, after a simple tuning we fixed the following values:
\begin{verbatim}
- VOCABULARY SIZE: 50000
- EMBEDDING SIZE: 300
- MAX SENTENCE LENGHT: 100
- LSTM HIDDEN STATE SIZE: 128
- NUMBER OF CONVOLUTIONAL FILTERS: 128
- CONVOLUTIONAL SLIDING WINDOW SIZE: 5
- BATCH SIZE: 64
- NUMBER OF EPOCHS: 4
- DROPOUT PROBABILITY: 0.2
\end{verbatim}

\paragraph*{Challenges, results and conclusions}
For each class we calculated precision, recall and f1 score obtaining the following results:
\begin{verbatim}
             precision    recall  f1-score   support

  non-burst       0.87      0.93      0.90       769
      burst       0.93      0.87      0.90       807

  avg/total       0.90      0.90      0.90      1576
\end{verbatim}

As we mentioned above one of the main challenges was to solve the initial unbalancing of the dataset. We did
so using an undersampling technique that could have been random but we chose a different solution. We used 
a method called NearMiss that adds some heuristics to the choice of samples. The mathematical formulation is
the following: let positive samples be the samples belonging to the targeted class to be under-sampled;
negative sample refers to the samples from the minority class (i.e., the most under-represented class).
NearMiss in its version 1 selects the positive samples for which the average distance to the N closest
samples of the negative class is the smallest.

Of course as always another challenge was the choice of the right hyperparameters. The performance overall
is pretty good especially with respect to the results of the original paper (0.80) considering that we
didn't use much data (about 8000 examples) and that these data where mostly automatically generated.
As often in deep learning having more data always improves performance especially if we want to add 
complexity to the network.
\end{document}
